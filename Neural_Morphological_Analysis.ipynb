{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Morphological Analysis Updates.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "417b64ac11504070b8fea24ac84eae97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8c5bbea10cae4329af3a9f50dc1a9ac7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1af2a39724674bcd9f642fa04efabe6c",
              "IPY_MODEL_57a1d8c961be403a8c91f1817bd43571",
              "IPY_MODEL_278df75616a44af4897f3c34543da0fa"
            ]
          }
        },
        "8c5bbea10cae4329af3a9f50dc1a9ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1af2a39724674bcd9f642fa04efabe6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fc83b402276b4e85bd901e2bdc58a23a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a677eb3464a42d4928e0ad387ee8bdc"
          }
        },
        "57a1d8c961be403a8c91f1817bd43571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dc5d48e5ca254478b666d61f1aa30ece",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9966,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9966,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7416cef12274ebcb12a3d2fc2988ed5"
          }
        },
        "278df75616a44af4897f3c34543da0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ddd5e6c6e9c4aa5881b9d49138e390b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9966/9966 [49:43&lt;00:00,  2.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f21b87b9d234de1b139463ab022a042"
          }
        },
        "fc83b402276b4e85bd901e2bdc58a23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a677eb3464a42d4928e0ad387ee8bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc5d48e5ca254478b666d61f1aa30ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7416cef12274ebcb12a3d2fc2988ed5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ddd5e6c6e9c4aa5881b9d49138e390b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f21b87b9d234de1b139463ab022a042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aRdxYgspfwd"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "zvv1C7YkZJPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "LLralfRgZM4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
      ],
      "metadata": {
        "id": "OeYxspDBZYI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
      ],
      "metadata": {
        "id": "HDbOR6YOdgJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F0XHj7ipZWH"
      },
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(\"gpu:0\"):\n",
        "   print(\"tf.keras code in this scope will run on GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGPyN7gl09fh",
        "outputId": "43268fc3-5d2d-44e5-ccf4-cbebf6276c8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.keras code in this scope will run on GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_NAME = \"upsampling\""
      ],
      "metadata": {
        "id": "OzgEUw6sL9z7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "prefix = \"/content/drive\"\n",
        "drive.mount(prefix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fr9ZLDHOIBb",
        "outputId": "5c3d89b8-500f-422f-a396-d022531d2dcf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix += \"/My Drive/Thesis\""
      ],
      "metadata": {
        "id": "-gyr8ZG4Qw8B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = prefix+\"/\"+TEST_NAME+\"/train.txt\"\n",
        "val_file = prefix+\"/\"+TEST_NAME+\"/dev.txt\"\n",
        "test_file = prefix+\"/\"+TEST_NAME+\"/test.txt\"\n",
        "dataset_sentences = prefix+\"/\"+\"parse_sentences.txt\""
      ],
      "metadata": {
        "id": "qGD2nXfUJDf8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(dataset_sentences) as f:\n",
        "  sent_dict = {}\n",
        "  lines = f.read().split(\"\\n\")  \n",
        "  for line in lines[:-1]:\n",
        "      sentence, id = line.split(\"\\t\")\n",
        "      sent_dict[id] = sentence"
      ],
      "metadata": {
        "id": "OSsGoeRhLTWC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sent_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcQTqTWGahbh",
        "outputId": "29e151f4-68c6-4aa9-fb30-f54af071ee10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4849"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(raw, id):\n",
        "\n",
        "  sentence = sent_dict[id]\n",
        "\n",
        "  input_ids = tokenizer(sentence)[\"input_ids\"]\n",
        "\n",
        "  encoded_input = tokenizer(sentence, return_tensors='pt')\n",
        "  output = model(**encoded_input)\n",
        "\n",
        "  idx = 0\n",
        "  for id_cand,id in enumerate(input_ids):\n",
        "    if tokenizer.decode([id]) == raw[:len(tokenizer.decode([id]))]:\n",
        "      idx = id_cand\n",
        "\n",
        "  return output['last_hidden_state'][0][idx]"
      ],
      "metadata": {
        "id": "LLVw3p42JSJd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj8G8KYlr_Pt"
      },
      "source": [
        "def process_dataset(data):\n",
        "  with open(data) as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  text_pairs = []\n",
        "  for line in lines:\n",
        "    if not(line):\n",
        "      continue\n",
        "    try:\n",
        "      token, morphs, id = line.split(\"\\t\")\n",
        "    except:\n",
        "      token, morphs = line.split(\"\\t\")\n",
        "    raw = \" \".join(token) # Char split\n",
        "    morphs = f\"{' '.join(morphs.split('+')[0])} {' '.join(morphs.split('+')[1:])}\" # Char split\n",
        "    morphs = \"[start] \" + morphs + \" [end]\"\n",
        "    #context = get_context(token, id)\n",
        "    #text_pairs.append((raw, morphs, context))\n",
        "    text_pairs.append((raw, morphs))\n",
        "  return text_pairs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = process_dataset(train_file)\n",
        "val_pairs = process_dataset(val_file)\n",
        "test_pairs = process_dataset(test_file)"
      ],
      "metadata": {
        "id": "dtxUs5nNHV2J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3N8H52g86uA",
        "outputId": "939fc5b0-fd6a-4bce-c8a8-cb1778da14b2"
      },
      "source": [
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56771 training pairs\n",
            "9966 validation pairs\n",
            "9966 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = set()\n",
        "for pair in train_pairs+val_pairs+test_pairs:\n",
        "  for c in pair[0].split():\n",
        "    char_set.add(c)\n",
        "  for c in pair[1].split():\n",
        "    char_set.add(c)\n",
        "\n",
        "print(len(char_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMu_OcWeEQiN",
        "outputId": "72d83870-cfb0-4567-a0b7-828d81c837d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_seq_length(pairs):\n",
        "    return max(max([len(token[0].split()[1:-1]) for token in pairs]),\n",
        "               max([len(token[0].split()[1:-1]) for token in pairs]))"
      ],
      "metadata": {
        "id": "V463gyhD2cfo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = max(get_max_seq_length(train_pairs),\n",
        "                     get_max_seq_length(val_pairs),\n",
        "                     get_max_seq_length(test_pairs))"
      ],
      "metadata": {
        "id": "fxaih-0m3IPI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDa1yRcbFdjG"
      },
      "source": [
        "## Dataset Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnYUbLk3te7U"
      },
      "source": [
        "vocab_size = len(char_set)\n",
        "sequence_length = max_seq_length\n",
        "batch_size = 64\n",
        "\n",
        "def custom_standardization_raw(input_string):\n",
        "    return input_string\n",
        "\n",
        "def custom_standardization_morph(input_string):\n",
        "    return input_string\n",
        "\n",
        "raw_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, \n",
        "    output_mode=\"int\", \n",
        "    output_sequence_length=sequence_length, \n",
        "    standardize=custom_standardization_raw\n",
        ")\n",
        "morph_vectorization = TextVectorization(\n",
        "    standardize=custom_standardization_morph,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        ")\n",
        "\n",
        "train_raw_texts = [pair[0] for pair in train_pairs]\n",
        "train_morph_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "raw_vectorization.adapt(train_raw_texts)\n",
        "morph_vectorization.adapt(train_morph_texts)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsR1QR0cku2H"
      },
      "source": [
        "import pickle\n",
        "pickle.dump({'config': raw_vectorization.get_config(),\n",
        "             'weights': raw_vectorization.get_weights()}\n",
        "            , open(f\"{TEST_NAME}_raw_vectorizator.pkl\", \"wb\"))\n",
        "\n",
        "pickle.dump({'config': morph_vectorization.get_config(),\n",
        "             'weights': morph_vectorization.get_weights()}\n",
        "            , open(f\"{TEST_NAME}_morph_vectorizator.pkl\", \"wb\"))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lba25Z9Avka2"
      },
      "source": [
        "def format_dataset(raw, morph):\n",
        "    raw = raw_vectorization(raw)\n",
        "    morph = morph_vectorization(morph)\n",
        "    return ({\"encoder_inputs\": raw, \"decoder_inputs\": morph[:, :-1]}, morph[:, 1:])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8UH68Znvnf9"
      },
      "source": [
        "def make_dataset(pairs):\n",
        "    raw_texts, morph_texts = zip(*pairs)\n",
        "    raw_texts = list(raw_texts)\n",
        "    morph_texts = list(morph_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((raw_texts, morph_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    if TEST_NAME == \"feat_dist\":\n",
        "      return dataset.shuffle(2048).prefetch(16).cache()\n",
        "    return dataset"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r9FStaLvm_g"
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y8Xp8MPpvT5",
        "outputId": "14abacb4-045f-46ff-dd33-d91ae3ac11d8"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 26)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 26)\n",
            "targets.shape: (64, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'token_embeddings': self.token_embeddings,\n",
        "            'position_embeddings': self.position_embeddings,\n",
        "            'sequence_length': self.sequence_length,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim,\n",
        "              })\n",
        "        return config\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "4yyyGIF4h_Hk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r2j2sCMFvJG"
      },
      "source": [
        "## Training Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U5jnzAkF0RP"
      },
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96fcYmPqsoJM",
        "outputId": "b1d37ccc-f9bc-4a83-fce5-f300ac1780fa"
      },
      "source": [
        "epochs = 5  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding_2 (Positi  (None, None, 256)   87296       ['encoder_inputs[0][0]']         \n",
            " onalEmbedding)                                                                                   \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Transfo  (None, None, 256)   3155456     ['positional_embedding_2[0][0]'] \n",
            " rmerEncoder)                                                                                     \n",
            "                                                                                                  \n",
            " model_3 (Functional)           (None, None, 315)    5427771     ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder_1[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,670,523\n",
            "Trainable params: 8,670,523\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "888/888 [==============================] - 48s 50ms/step - loss: 0.0825 - accuracy: 0.9407 - val_loss: 0.3915 - val_accuracy: 0.8393\n",
            "Epoch 2/5\n",
            "888/888 [==============================] - 43s 49ms/step - loss: 0.0727 - accuracy: 0.9477 - val_loss: 0.2816 - val_accuracy: 0.8803\n",
            "Epoch 3/5\n",
            "888/888 [==============================] - 44s 49ms/step - loss: 0.0664 - accuracy: 0.9521 - val_loss: 0.2697 - val_accuracy: 0.8799\n",
            "Epoch 4/5\n",
            "888/888 [==============================] - 43s 49ms/step - loss: 0.0610 - accuracy: 0.9557 - val_loss: 0.2546 - val_accuracy: 0.8807\n",
            "Epoch 5/5\n",
            "888/888 [==============================] - 44s 49ms/step - loss: 0.0569 - accuracy: 0.9589 - val_loss: 0.2235 - val_accuracy: 0.9016\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9252adeed0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0yhxeqbjVgb5",
        "outputId": "0a3899ae-6c7c-4dfb-ce53-7a6ef975f5db"
      },
      "source": [
        "from google.colab import files\n",
        "transformer.save_weights(f\"{TEST_NAME}\")\n",
        "files.download(f\"{TEST_NAME}_raw_vectorizator.pkl\")\n",
        "files.download(f\"{TEST_NAME}_morph_vectorizator.pkl\")\n",
        "files.download(f\"{TEST_NAME}.index\")\n",
        "files.download(f\"{TEST_NAME}.data-00000-of-00001\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9e72edb4-8bc4-404e-9b4e-c6c5ed1e795f\", \"upsampling_raw_vectorizator.pkl\", 1461)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bc1707d4-c0a5-4558-b0c3-fd50e59d9abe\", \"upsampling_morph_vectorizator.pkl\", 4190)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2c8e4922-c8d4-4ee1-84d4-2cdf192dd652\", \"upsampling.index\", 8179)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c3cf4c7d-0d2e-43b0-928d-6e7a9e9c2e48\", \"upsampling.data-00000-of-00001\", 69384991)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT4ufJPbssEq"
      },
      "source": [
        "morph_vocab = morph_vectorization.get_vocabulary()\n",
        "morph_index_lookup = dict(zip(range(len(morph_vocab)), morph_vocab))\n",
        "max_decoded_sentence_length = 25"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM026WYRte2D"
      },
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = raw_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = morph_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = morph_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "          break\n",
        "    return decoded_sentence"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_unique = ['NOUN', 'VERB', 'PUNCT', 'ADJ', 'CONJ^Coor', 'ADP^CBare', 'NUM', 'ADV',\n",
        "       'DET^Ind', 'PRT', 'PRON', 'DET^Dem', 'NOUN^Temporal', 'NOUN^PROP', '^PRON',\n",
        "       'ADP^CDat', 'X', 'CONJ^Adv', 'DET^Def', 'DET', 'ADP^CAbl', 'ADP^CGen',\n",
        "       'ADP^CIns', 'CONJ^Par', 'ADV^Temporal', 'ADP^CNum', 'ADP^CFin',\n",
        "       'CONJ^Sub', 'ONOM']"
      ],
      "metadata": {
        "id": "AUXT4XHutTrW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKFtc18LsvRN"
      },
      "source": [
        "def normalize(input):\n",
        "  pre = \"\"\n",
        "  feats = \"\"\n",
        "  tokens = input.split()[1:-1]\n",
        "  match = False\n",
        "  for idx,token in enumerate(tokens):\n",
        "    if token in pos_unique and not match:\n",
        "      pre += \"\".join(tokens[0:idx])+\"+\"+token+\"+\"\n",
        "      match=True\n",
        "    elif match:\n",
        "      feats += token+\"+\"\n",
        "  return (pre+feats).strip(\"+\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "test_raw_texts = [pair[0] for pair in test_pairs]\n",
        "test_morph_texts = [pair[1] for pair in test_pairs]"
      ],
      "metadata": {
        "id": "XUaf6rL34EqS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_morph_texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sGcHFU0OokNn",
        "outputId": "e9fbfc22-9cc4-4edd-8d77-c8cfd6ae3d5e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[start] t h y n NOUN Apostrophe=True PersonNumber=A3pl Possessive=Pnon Case=Nom Proper=True [end]'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 20\n",
        "print(normalize(test_morph_texts[idx]))\n",
        "print(test_pairs[idx])"
      ],
      "metadata": {
        "id": "WwBke383-KxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7b939b-6825-49f3-da04-65b1225a0dba"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elçi+NOUN+PersonNumber=A3sg+Possessive=Pnon+Case=Bare+Proper=True+DB^NOUN+Derivation=Ness+PersonNumber=A3sg+Possessive=P3sg+Case=Nom+Proper=True\n",
            "('e l ç i l i ğ i', '[start] e l ç i NOUN PersonNumber=A3sg Possessive=Pnon Case=Bare Proper=True DB^NOUN Derivation=Ness PersonNumber=A3sg Possessive=P3sg Case=Nom Proper=True [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3I96urctcXC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "417b64ac11504070b8fea24ac84eae97",
            "8c5bbea10cae4329af3a9f50dc1a9ac7",
            "1af2a39724674bcd9f642fa04efabe6c",
            "57a1d8c961be403a8c91f1817bd43571",
            "278df75616a44af4897f3c34543da0fa",
            "fc83b402276b4e85bd901e2bdc58a23a",
            "6a677eb3464a42d4928e0ad387ee8bdc",
            "dc5d48e5ca254478b666d61f1aa30ece",
            "c7416cef12274ebcb12a3d2fc2988ed5",
            "5ddd5e6c6e9c4aa5881b9d49138e390b",
            "3f21b87b9d234de1b139463ab022a042"
          ]
        },
        "outputId": "9714a735-4b60-4628-f0e5-f7e579e2d481"
      },
      "source": [
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with open(f'{TEST_NAME}.txt', 'w+', encoding='utf-8') as f_out:\n",
        "  for i in tqdm_notebook(range(max(len(test_raw_texts),len(test_morph_texts)))):\n",
        "    try:\n",
        "      pred = normalize(decode_sequence(test_raw_texts[i]))\n",
        "      true = normalize(test_morph_texts[i])\n",
        "\n",
        "      y_true.append(true)\n",
        "      y_pred.append(pred)\n",
        "\n",
        "      f_out.write(f\"{pred}\\t{true}\\n\")\n",
        "    except:\n",
        "      print(\"Error\")\n",
        "      sys.exit()\n",
        "\n",
        "files.download(f'{TEST_NAME}.txt')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "417b64ac11504070b8fea24ac84eae97",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/9966 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3d8a9a5a-8e54-4e88-bbdc-74ec1623073f\", \"upsampling.txt\", 918060)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [true.split(\"+\") for true in y_true]\n",
        "y_pred = [pred.split(\"+\") for pred in y_pred]"
      ],
      "metadata": {
        "id": "dR-3mMUDns7C"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true[0]"
      ],
      "metadata": {
        "id": "TfemHDJzf-nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [x if len(x)>1 else [\".\"]+x for x in y_pred]\n",
        "y_true = [x if len(x)>1 else [\".\"]+x for x in y_true]"
      ],
      "metadata": {
        "id": "Va6jmFZif3-V"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLyelv5DFN_X"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_BLEU():\n",
        "  total_score = 0\n",
        "  for pred, true in zip(y_pred,y_true):\n",
        "    reference = [pred][:]\n",
        "    candidate = true[:]\n",
        "    score = sentence_bleu(reference, candidate)\n",
        "    \n",
        "    total_score += score\n",
        "\n",
        "  return total_score/len(y_true)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMz03jtRnlv"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def calculate_lemma():\n",
        "\n",
        "  true = [token[0] for token in y_true]\n",
        "  pred = [token[0] for token in y_pred]\n",
        "\n",
        "  return accuracy_score(true,pred)\n",
        "\n",
        "def calculate_POS():\n",
        "  true = [token[1] for token in y_true]\n",
        "  pred = [token[1] for token in y_pred]\n",
        "\n",
        "  return accuracy_score(true,pred)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW3axlC0tpBJ"
      },
      "source": [
        "def calculate_precision_and_recall(y_true,y_pred):\n",
        "\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "\n",
        "    for true,pred in zip(y_true,y_pred):\n",
        "        try:\n",
        "            max_precision = 0\n",
        "            max_recall = 0\n",
        "\n",
        "            x = true\n",
        "            y = pred\n",
        "\n",
        "            if len(x) > 0 and len(y) > 0:\n",
        "                shared_items = dict()\n",
        "                for k in x:\n",
        "                    if (k in y) and (x.index(k) == y.index(k)):\n",
        "                        shared_items[k] = x.index(k)\n",
        "\n",
        "                recall = len(shared_items)/len(x)\n",
        "                precision = len(shared_items)/len(y)\n",
        "\n",
        "                if precision > max_precision:\n",
        "                    max_precision = precision\n",
        "\n",
        "                if recall > max_recall:\n",
        "                    max_recall = recall\n",
        "\n",
        "            precisions.append(max_precision)\n",
        "            recalls.append(max_recall)\n",
        "\n",
        "        except KeyError:\n",
        "            precisions.append(0)\n",
        "            recalls.append(0)\n",
        "\n",
        "\n",
        "    sum = 0\n",
        "    for item in precisions:\n",
        "        sum += item\n",
        "\n",
        "    average_precision = sum/len(precisions)\n",
        "\n",
        "    sum = 0\n",
        "\n",
        "    for item in recalls:\n",
        "        sum += item\n",
        "\n",
        "    average_recall = sum/len(precisions)\n",
        "\n",
        "    \n",
        "    return average_precision, average_recall"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRjFO0jsGv9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0073fd6-b3cb-4bd0-ddad-0bbf299b6d29"
      },
      "source": [
        "bleu_score = calculate_BLEU()\n",
        "precision, recall = calculate_precision_and_recall(y_true,y_pred)\n",
        "pos_score = calculate_POS()\n",
        "lemma_score = calculate_lemma()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "bnLycKqMjxxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"POS Accuracy:\", pos_score)\n",
        "print(\"Lemma Accuracy:\", lemma_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fTnKYvXZeFc",
        "outputId": "b3f96cec-720e-4588-b342-49a99ae35f53"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7837730515107862\n",
            "Recall: 0.7844898273845623\n",
            "BLEU Score: 0.81305067886416\n",
            "POS Accuracy: 0.8554083885209713\n",
            "Lemma Accuracy: 0.8404575556893438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phonological Normalization"
      ],
      "metadata": {
        "id": "bGkb0Szfj1U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"POS Accuracy:\", pos_score)\n",
        "print(\"Lemma Accuracy:\", lemma_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27NtVzQSj4L5",
        "outputId": "6f13e408-46f0-4b7f-be35-13b0ddf93702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.839238831598399\n",
            "Recall: 0.8384282312829477\n",
            "BLEU Score: 0.8603016822291351\n",
            "POS Accuracy: 0.8866144892634958\n",
            "Lemma Accuracy: 0.9408990567930965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Normalization"
      ],
      "metadata": {
        "id": "_E0vkXHwOQVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"POS Accuracy:\", pos_score)\n",
        "print(\"Lemma Accuracy:\", lemma_score)"
      ],
      "metadata": {
        "id": "VI71MvnDj6Ud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bdef80-02d5-4a9e-8f0b-95598780046d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7846768618141273\n",
            "Recall: 0.7855815146679123\n",
            "BLEU Score: 0.8338185955282936\n",
            "POS Accuracy: 0.8535019064820389\n",
            "Lemma Accuracy: 0.8520971302428256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Distribution"
      ],
      "metadata": {
        "id": "e36mseRdyrCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"POS Accuracy:\", pos_score)\n",
        "print(\"Lemma Accuracy:\", lemma_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NORkKIDtOVDr",
        "outputId": "fcd5ad1c-1489-4805-d0ba-510e56876b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.5920176086009074\n",
            "Recall: 0.583141563153986\n",
            "BLEU Score: 0.6326301739790313\n",
            "POS Accuracy: 0.7238611278346377\n",
            "Lemma Accuracy: 0.6385711418824002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upsampling"
      ],
      "metadata": {
        "id": "pPdkYYJPjuAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"POS Accuracy:\", pos_score)\n",
        "print(\"Lemma Accuracy:\", lemma_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_I4_N4Njugb",
        "outputId": "5aad7ceb-a45d-48eb-f5a5-bffd0155a8d5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.40824612655102005\n",
            "Recall: 0.3722607563853567\n",
            "BLEU Score: 0.6904352492158221\n",
            "POS Accuracy: 0.31015452538631344\n",
            "Lemma Accuracy: 0.5749548464780253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Process"
      ],
      "metadata": {
        "id": "E-tc4jRMZhGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"POS Accuracy:\", pos_score)\n",
        "print(\"Lemma Accuracy:\", lemma_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0gRIEtaZezX",
        "outputId": "c40a888c-88d1-4242-f661-57bb0be8ba5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8556059411621291\n",
            "Recall: 0.8584968944917806\n",
            "BLEU Score: 0.891825792284436\n",
            "POS Accuracy: 0.8385510736504114\n",
            "Lemma Accuracy: 0.9435079269516355\n"
          ]
        }
      ]
    }
  ]
}