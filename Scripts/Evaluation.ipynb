{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = confusion_matrix(true_pos_output,\n",
    "                              model_pos_output,\n",
    "                              labels=list(pd.Series(true_pos_output).value_counts().index),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12,7]\n",
    "sns.heatmap(pd.DataFrame(clf_report), \n",
    "            annot=True,\n",
    "            xticklabels=list(pd.Series(true_pos_output).value_counts().index), \n",
    "            yticklabels=list(pd.Series(model_pos_output).value_counts().index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_total_output[2]\n",
    "model_total_output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_precision(y_pred,y_true):\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for true,pred in zip(y_true,y_pred):\n",
    "        try:\n",
    "            max_precision = 0\n",
    "            max_recall = 0\n",
    "\n",
    "            x = true\n",
    "            y = pred\n",
    "\n",
    "            if len(x) > 0 and len(y) > 0:\n",
    "                shared_items = dict()\n",
    "                for k in x:\n",
    "                    if (k in y) and (x.index(k) == y.index(k)):\n",
    "                        shared_items[k] = x.index(k)\n",
    "\n",
    "                recall = len(shared_items)/len(x)\n",
    "                precision = len(shared_items)/len(y)\n",
    "\n",
    "                if precision > max_precision:\n",
    "                    max_precision = precision\n",
    "\n",
    "                if recall > max_recall:\n",
    "                    max_recall = recall\n",
    "\n",
    "            precisions.append(max_precision)\n",
    "            recalls.append(max_recall)\n",
    "\n",
    "        except KeyError:\n",
    "            precisions.append(0)\n",
    "            recalls.append(0)\n",
    "\n",
    "\n",
    "    sum = 0\n",
    "    for item in precisions:\n",
    "        sum += item\n",
    "    print('average_precision =', sum/len(precisions)) \n",
    "    average_precision = sum/len(precisions)\n",
    "\n",
    "    sum = 0\n",
    "\n",
    "    for item in recalls:\n",
    "        sum += item\n",
    "\n",
    "    print('average_recall =', sum/len(recalls))\n",
    "    average_recall = sum/len(precisions)\n",
    "\n",
    "    \n",
    "    return average_precision, average_recall\n",
    "precision,recall = calculate_precision(model_total_output,true_total_output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
